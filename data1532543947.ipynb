{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn\n",
    "\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "init_notebook_mode(connected=True)\n",
    "import random\n",
    "import math\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Global Constants\n",
    "categorical = 'categorical'\n",
    "numerical = 'numerical'\n",
    "categorical_null_val = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_datatype_util(col_datatype):\n",
    "    return dict( [(col,str) if val==categorical else (col,float) for col,val in col_datatype.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_datatype():\n",
    "    col_datatype = {\"partner_id\":categorical,\"domain_name\":categorical,\"country_code\":categorical,\"creative_id\":categorical,\"canonical_hash\":categorical,\"keyword_strategy_used\":categorical,\"browser_id\":categorical,\"device_id\":categorical,\"os_id\":categorical,\"metro_code\":categorical,\"url_category_id\":categorical,\"hour_id\":categorical}\n",
    "\n",
    "    return col_datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_default(df):\n",
    "    return dict([(col,categorical_null_val) if val==categorical else (col,round(df[col].mean(),2)) for col,val in get_col_datatype().items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\nishant.da\\Downloads\\nishant_dayal_visible_data\\\\data.csv\",\n",
    "                 dtype = get_col_datatype_util(get_col_datatype()))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"visible_impression\"\n",
    "weight_col = \"impression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Constants\n",
    "misc_percent = 0.005\n",
    "misc_col_value = 'Others'\n",
    "col_datatype = get_col_datatype()\n",
    "col_default = get_col_default(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weight_col is None:\n",
    "    weight_col='weight'\n",
    "    df[weight_col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    return [x for x in list(df.columns) if x not in [target_col,weight_col]]\n",
    "    \n",
    "def get_categorical_cols():\n",
    "    return [x for x,v in col_datatype.items() if v==categorical]\n",
    "    \n",
    "def get_numerical_cols():\n",
    "    return [x for x,v in col_datatype.items() if v==numerical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_colors(num_of_colors):\n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                 for i in range(num_of_colors)]\n",
    "    return color\n",
    "    \n",
    "def make_copy_df(df):\n",
    "    return df.copy()\n",
    "    \n",
    "def get_total(df,col):\n",
    "    return df[col].sum()\n",
    "    \n",
    "def fill_default_values(df):\n",
    "    for c in get_features(df):\n",
    "        df[c].fillna(col_default.get(c),inplace=True)\n",
    "    return df\n",
    "    \n",
    "def return_top_k(df,col,top_k):\n",
    "    temp_df = df.sort_values(by=col,ascending=False)\n",
    "    return temp_df[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique Values DataFrame\n",
    "def unique_count(df):\n",
    "    feature_col = 'Features'\n",
    "    count_col = 'Unique Count'\n",
    "    unique_count_df = pd.DataFrame(columns=[feature_col,count_col])\n",
    "    \n",
    "    unique_count_df[feature_col] = get_categorical_cols()\n",
    "    unique_count_df[count_col] = unique_count_df[feature_col].apply(lambda col: df[col].nunique())\n",
    "    \n",
    "    return unique_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Count DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count_df = unique_count(df)\n",
    "unique_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the dataframe subset and fill NULL values with some other value\n",
    "def make_group(df,col,weight_col,fill_na=False,reset_index=True):\n",
    "    temp_df = pd.DataFrame(df[col+[weight_col]])\n",
    "    \n",
    "    if fill_na:\n",
    "        temp_df = fill_default_values(temp_df)\n",
    "    \n",
    "    group = temp_df.groupby(col).agg({weight_col:'sum'})\n",
    "    \n",
    "    if reset_index:\n",
    "        group = group.reset_index()\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_miscing(df,col,weight_col,misc_percent):\n",
    "    group = make_group(df,[col],weight_col)\n",
    "\n",
    "    if_misc_col = 'if_misc'\n",
    "    group[if_misc_col]=False\n",
    "    \n",
    "    summation = get_total(df,weight_col)*misc_percent*0.01\n",
    "    \n",
    "    group[if_misc_col] = group[weight_col].apply(lambda x:True if x<summation else False)\n",
    "    group[col] = group.apply(lambda x:misc_col_value if x[if_misc_col] else x[col],axis=1)\n",
    "    \n",
    "    misced_group = make_group(group,[col],weight_col)\n",
    "    return misced_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_feature_irrelevant(df,col,weight_col,misc_percent):    \n",
    "    fin_group = do_miscing(df,col,weight_col,misc_percent)\n",
    "    fin_group = fin_group[(fin_group[col]!=col_default.get(col)) & (fin_group[col]!=misc_col_value)]\n",
    "    \n",
    "    return fin_group.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irrelevant_features(df,weight_col,misc_precent):\n",
    "    irrelevant_cols=[]\n",
    "    \n",
    "    for col in get_features(df):\n",
    "        if df[col].nunique()==df.shape[0]:\n",
    "            irrelevant_cols.append(col)\n",
    "        elif is_feature_irrelevant(df,col,weight_col,0.05):\n",
    "            irrelevant_cols.append(col)\n",
    "            \n",
    "    return irrelevant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrelevant_features(df,weight_col,misc_percent):\n",
    "    irrelevant_features = get_irrelevant_features(df,weight_col,misc_percent)\n",
    "    df.drop(irrelevant_features,axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_irrelevant_features(df,weight_col,misc_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_misced_df(df,target_col,weight_col):\n",
    "    df = make_copy_df(df)\n",
    "    df = fill_default_values(df)\n",
    "    \n",
    "    misced_df = pd.DataFrame(columns = list(df.columns))\n",
    "    misced_df[target_col] = df[target_col]\n",
    "    misced_df[weight_col] = df[weight_col]\n",
    "    \n",
    "    for col in get_categorical_cols():\n",
    "        misced_group = do_miscing(df,col,weight_col,misc_percent)\n",
    "        unique_values = set(misced_group[col].unique())\n",
    "        misced_df[col] = df[col].apply(lambda x: x if (x in unique_values) else misc_col_value)\n",
    "    \n",
    "    for col in get_numerical_cols():\n",
    "        misced_df[col] = df[col]\n",
    "        \n",
    "    return misced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misced DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misced_df = make_misced_df(make_copy_df(df),target_col,weight_col)\n",
    "misced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bar_trace(x,y,name=''):\n",
    "    return go.Bar(x = x,y = y,name = name,opacity=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(x,y,title):\n",
    "    trace = get_bar_trace(x,y)\n",
    "    data = [trace]\n",
    "    layout = go.Layout(title=title,xaxis=dict(type='category'))\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_missing_count(df,col,weight_col):\n",
    "    temp_df = df[df[col].isnull()][weight_col]\n",
    "    return temp_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Values Plot (Count and Ratio)\n",
    "def missing_values_plot(df,weight_col):\n",
    "    feature_col = 'Features'\n",
    "    count_col = 'Count'\n",
    "    ratio_col = 'Ratio'\n",
    "    \n",
    "    missing = pd.DataFrame(columns=[feature_col,count_col,ratio_col])\n",
    "    \n",
    "    missing[feature_col]=get_features(df)\n",
    "    missing[count_col] = missing[feature_col].apply(lambda col: calculate_missing_count(df,col,weight_col))\n",
    "    \n",
    "    total_weight = get_total(df,weight_col)\n",
    "    \n",
    "    missing[ratio_col] = missing[count_col].apply(lambda x:x/total_weight) \n",
    "    missing.sort_values(by=count_col,ascending=False,inplace=True)\n",
    "    \n",
    "    plot_bar_chart(missing[feature_col],missing[count_col],'Missing Count')\n",
    "    plot_bar_chart(missing[feature_col],missing[ratio_col],'Missing Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Count and Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_plot(make_copy_df(df),weight_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plt_params(width,height,title,xlabel,ylabel):\n",
    "    plt.rcParams[\"figure.figsize\"] = [width,height]\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(labels,values,title):\n",
    "    data = [go.Pie(\n",
    "        labels=labels,\n",
    "        values=values,\n",
    "        marker=dict( colors = random_colors(len(labels)) ),\n",
    "        textfont=dict( size = 20 )\n",
    "    )]    \n",
    "    layout = go.Layout(title = title)\n",
    "    \n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_chart(df,col,weight_col,top_k):\n",
    "    title = col\n",
    "    \n",
    "    group = make_group(df,[col],weight_col)\n",
    "    group_top_k = return_top_k(group,weight_col,top_k)\n",
    "    \n",
    "    set_values = set(group_top_k[col])\n",
    "    group[col] = group[col].apply(lambda x:misc_col_value if x not in set_values else x)\n",
    "    \n",
    "    group = make_group(group,[col],weight_col)\n",
    "    \n",
    "    plot_pie_chart(group[col],group[weight_col],title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(df,val,col,weight_col):\n",
    "    index = df[weight_col].searchsorted(val)\n",
    "    ret_df = df.iloc[index].reset_index()\n",
    "    \n",
    "    return (ret_df.iloc[0][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_first_and_third_quartile(arr,df,total_len,factor,col,weight_col):\n",
    "    if ((total_len+1)%4)==0:\n",
    "        arr.append(get_value(df,(total_len+1)*factor,col,weight_col))\n",
    "    else:\n",
    "        arr.append((get_value(df,math.ceil((total_len+1)*factor),col,weight_col)+get_value(df,math.floor((total_len+1)*factor),col,weight_col))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quartiles(df,col,weight_col):\n",
    "    df = make_copy_df(df)\n",
    "    df.sort_values(by=col,inplace=True)\n",
    "    df.reset_index()\n",
    "    df[weight_col] = df[weight_col].cumsum()\n",
    "    \n",
    "    total_len = df.iloc[df.shape[0]-1][weight_col]\n",
    "    arr = list()\n",
    "    \n",
    "    arr.append(df.iloc[0][col])\n",
    "    append_first_and_third_quartile(arr,df,total_len,1/4,col,weight_col)\n",
    "    \n",
    "    if total_len%2==1:\n",
    "        arr.append((get_value(df,total_len/2,col,weight_col)+get_value(df,(total_len/2)+1,col,weight_col))/2)\n",
    "    else:\n",
    "        arr.append(get_value(df,(total_len+1)/2,col,weight_col))\n",
    "    \n",
    "    append_first_and_third_quartile(arr,df,total_len,3/4,col,weight_col)\n",
    "    arr.append(df.iloc[df.shape[0]-1][col])\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df,col,weight_col):\n",
    "    quar = quartiles(df,col,weight_col)\n",
    "    iqr_range = quar[3]-quar[1]\n",
    "    \n",
    "    df = df[df[col]>(quar[1]-1.5*iqr_range)]\n",
    "    df = df[df[col]<(quar[3]+1.5*iqr_range)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(df,col,weight_col,nbins):\n",
    "    title = col\n",
    "    \n",
    "    temp_df = df[[col,weight_col]]\n",
    "    temp_df = fill_default_values(temp_df)\n",
    "    temp_df = remove_outliers(temp_df,col,weight_col)\n",
    "\n",
    "    set_plt_params(20,15,title,col,'Weight')\n",
    "    plt.hist(temp_df[col],bins=nbins,weights=temp_df[weight_col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_plot(df,col,weight_col):\n",
    "    for c in col:\n",
    "        if col_datatype.get(c)==categorical:\n",
    "            pie_chart(df,c,weight_col,100)\n",
    "        elif col_datatype.get(c)==numerical:\n",
    "            hist_plot(df,c,weight_col,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_bar_chart(df,col,target_col,weight_col,title,top_k):\n",
    "    df_target_one = df[df[target_col]==1]\n",
    "    df_target_zero = df[df[target_col]==0]\n",
    "\n",
    "    trace_target_zero = get_bar_trace(df_target_zero[col],df_target_zero[weight_col],'Not Visible')\n",
    "    trace_target_one = get_bar_trace(df_target_one[col],df_target_one[weight_col],'Visible')\n",
    "\n",
    "    data = [trace_target_zero,trace_target_one]\n",
    "    layout = go.Layout(barmode='group',title=title,xaxis=dict(type='category'))\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_feature_value(df,col,target_col,weight_col,top_k):\n",
    "    group_with_weight = return_top_k(make_group(df,[col],weight_col),weight_col,top_k)\n",
    "    \n",
    "    group_with_target_and_weight = make_group(df,[col,target_col],weight_col)\n",
    "    \n",
    "    group = group_with_target_and_weight[group_with_target_and_weight[col].isin(group_with_weight[col])]\n",
    "    group = group[(group[col]!=col_default.get(col)) & (group[col]!=misc_col_value)]\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BarPlot of the classes of a category for Visible and Not Visible Target\n",
    "def bar_plot(df,col,target_col,weight_col,top_k):\n",
    "    title_name = col\n",
    "    \n",
    "    group = get_topk_feature_value(df,col,target_col,weight_col,top_k)\n",
    "    \n",
    "    plot_group_bar_chart(group,col,target_col,weight_col,title_name,top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot_target(df,col,target_col,weight_col,nbins):\n",
    "    title = col\n",
    "\n",
    "    temp_df = df[[col,target_col,weight_col]]\n",
    "    temp_df = fill_default_values(temp_df)\n",
    "    temp_df = remove_outliers(temp_df,col,weight_col)\n",
    "    \n",
    "    temp_df_target_one = temp_df[temp_df[target_col]==1]\n",
    "    temp_df_target_zero = temp_df[temp_df[target_col]==0]\n",
    "\n",
    "    set_plt_params(12,9,'Hist 0/1',col,'Weight')\n",
    "    plt.hist(temp_df_target_zero[col], bins=nbins, alpha=0.5, label='0',weights=temp_df_target_zero[weight_col])\n",
    "    plt.hist(temp_df_target_one[col], bins=nbins, alpha=0.5, label='1',weights=temp_df_target_one[weight_col])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_plot_target(df,col,target_col,weight_col):\n",
    "    for c in col:\n",
    "        if col_datatype.get(c)==categorical:\n",
    "            bar_plot(df,c,target_col,weight_col,100)\n",
    "        elif col_datatype.get(c)==numerical:\n",
    "            hist_plot_target(df,c,target_col,weight_col,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_values(df,col,target_col,weight_col,top_k):\n",
    "    group = get_topk_feature_value(df,col,target_col,weight_col,top_k)  \n",
    "    columns = set(group[col].unique())\n",
    "    \n",
    "    group = group.set_index([col,target_col])\n",
    "    group = group.to_dict()\n",
    "    \n",
    "    values = dict()\n",
    "    for i in columns:\n",
    "        val1 = group.get(weight_col).get((i,0)) if group.get(weight_col).get((i,0)) is not None else 0\n",
    "        val2 = group.get(weight_col).get((i,1)) if group.get(weight_col).get((i,1)) is not None else 0\n",
    "        values[i] = 0 if val2==0 else val2/(val1+val2)\n",
    "\n",
    "    values.pop(col_default.get(col),None)\n",
    "    values.pop(misc_col_value,None)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Encoded Value of each feature value in a feature\n",
    "def avg_val_bar_plot(df,col,target_col,weight_col,top_k):\n",
    "    values = get_avg_values(df,col,target_col,weight_col,top_k)\n",
    "    \n",
    "    sorted_values = sorted(values.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    sorted_x = [x[0] for x in sorted_values]\n",
    "    sorted_y = [x[1] for x in sorted_values]\n",
    "    \n",
    "    plot_bar_chart(sorted_x,sorted_y,'Average Target Value : '+str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_plot(df,col,weight_col,nbins):\n",
    "    title = col\n",
    "\n",
    "    temp_df = df[[col,weight_col]]\n",
    "    temp_df = fill_default_values(temp_df)\n",
    "    temp_df = remove_outliers(temp_df,col,weight_col)\n",
    "\n",
    "    ax = sns.distplot(temp_df[col],kde=True,bins=nbins,\n",
    "                 hist_kws={'weights': temp_df[weight_col]})\n",
    "    ax.set(xlabel=col, ylabel='PDF Value',title=title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_value_plot(df,col,target_col,weight_col):\n",
    "    for c in col:\n",
    "        if col_datatype.get(c)==categorical:\n",
    "            avg_val_bar_plot(df,c,target_col,weight_col,500)\n",
    "        elif col_datatype.get(c)==numerical:\n",
    "            dist_plot(df,c,weight_col,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_util(df,title,col,weight_col,group_col=None):\n",
    "    df = make_copy_df(df)\n",
    "    data = []\n",
    "    \n",
    "    if group_col==None:\n",
    "        arr = quartiles(df,col,weight_col)\n",
    "        arr.insert(int(len(arr)/2),arr[int(len(arr)/2)])\n",
    "        data.append(go.Box(y=arr))\n",
    "    else:\n",
    "        for c in df[group_col].unique():\n",
    "            arr = quartiles(df[df[group_col]==c],col,weight_col)\n",
    "            arr.insert(int(len(arr)/2),arr[int(len(arr)/2)])\n",
    "            data.append(go.Box(y=arr,name=c))\n",
    "    \n",
    "    layout = go.Layout(title = title)\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "    \n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(df,columns,weight_col,col_datatype,group_col=None):\n",
    "    for col in columns:\n",
    "        if col_datatype.get(col)==numerical:\n",
    "            box_plot_util(df,'Box Plot',col,weight_col,group_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Numerical DF\n",
    "def cat_to_numerical_TBS(col,prior,default_val,group,weight_col,include_na=True):\n",
    "    if (include_na) & (col==default_val):\n",
    "        return None\n",
    "    \n",
    "    weight_target_zero = group.get(weight_col).get((col, 0)) if group.get(weight_col).get((col, 0)) is not None else 0\n",
    "    weight_target_one = group.get(weight_col).get((col, 1)) if group.get(weight_col).get((col, 1)) is not None else 0\n",
    "    \n",
    "    summation = weight_target_zero + weight_target_one + 1\n",
    "    \n",
    "    retval = (weight_target_one+prior)/summation\n",
    "    return retval\n",
    "\n",
    "vectorized_cat_to_numerical_TBS = np.vectorize(cat_to_numerical_TBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior(df,target_col,weight_col):\n",
    "    group = make_group(df,[target_col],weight_col,reset_index=False).to_dict()\n",
    "    prior = group.get(weight_col).get(1)/((group.get(weight_col).get(0)+group.get(weight_col).get(1)))\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_numerical_df(df,target_col,weight_col,include_na=True):\n",
    "    df = make_copy_df(df)\n",
    "    df = fill_default_values(df)\n",
    "    \n",
    "    numerical_df = pd.DataFrame(columns = list(df.columns))\n",
    "    numerical_df[target_col] = df[target_col]\n",
    "    numerical_df[weight_col] = df[weight_col]\n",
    "    \n",
    "    prior = calculate_prior(df,target_col,weight_col)\n",
    "    \n",
    "    for col in get_categorical_cols():\n",
    "        group = make_group(df,[col,target_col],weight_col,reset_index=False).to_dict()\n",
    "        default_val = col_default.get(col)\n",
    "        numerical_df[col] = vectorized_cat_to_numerical_TBS(df[col],prior,default_val,group,weight_col,include_na)\n",
    "    \n",
    "    for col in get_numerical_cols():\n",
    "        numerical_df[col] = df[col]\n",
    "        \n",
    "    return numerical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df_without_na = make_numerical_df(misced_df,target_col,weight_col,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight of Evidence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_numerical_WOE(col,prior_zero,prior_one,default_val,group,weight_col,total_zero,total_one,capval,include_na=True):\n",
    "    if (include_na) & (col==default_val):\n",
    "        return None\n",
    "    \n",
    "    weight_target_zero = group.get(weight_col).get((col, 0)) if group.get(weight_col).get((col, 0)) is not None else 0\n",
    "    weight_target_one = group.get(weight_col).get((col, 1)) if group.get(weight_col).get((col, 1)) is not None else 0\n",
    "    \n",
    "    retval = math.log(((weight_target_one+prior_one)/total_one)/((weight_target_zero+prior_zero)/total_zero))\n",
    "\n",
    "    if retval>capval:\n",
    "        retval = capval\n",
    "    elif retval<(-capval):\n",
    "        retval = -capval\n",
    "        \n",
    "    return retval\n",
    "\n",
    "vectorized_cat_to_numerical_WOE = np.vectorize(cat_to_numerical_WOE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior_WOE(df,target_col,weight_col):\n",
    "    prior_one = (df[df[target_col]==1][weight_col]).sum()/df[weight_col].sum()\n",
    "    prior_zero = (df[df[target_col]==0][weight_col]).sum()/df[weight_col].sum()\n",
    "    return (prior_zero,prior_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_woe_df(df,target_col,weight_col,include_na=True):\n",
    "    df = make_copy_df(df)\n",
    "    df = fill_default_values(df)\n",
    "    \n",
    "    woe_df = pd.DataFrame(columns = list(df.columns))\n",
    "    woe_df[target_col] = df[target_col]\n",
    "    woe_df[weight_col] = df[weight_col]\n",
    "    \n",
    "    prior_zero,prior_one = calculate_prior_WOE(df,target_col,weight_col)\n",
    "    total_one = df[df[target_col]==1][weight_col].sum()\n",
    "    total_zero = df[df[target_col]==0][weight_col].sum()\n",
    "    \n",
    "    for col in get_categorical_cols():\n",
    "        group = make_group(df,[col,target_col],weight_col,reset_index=False).to_dict()\n",
    "        default_val = col_default.get(col)\n",
    "        woe_df[col] = vectorized_cat_to_numerical_WOE(df[col],prior_zero,prior_one,default_val,group,weight_col,total_zero,total_one,10,include_na)\n",
    "    \n",
    "    for col in get_numerical_cols():\n",
    "        woe_df[col] = df[col]\n",
    "        \n",
    "    return woe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight of Evidence DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_df = make_woe_df(misced_df,target_col,weight_col,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(df,col, weight_col):\n",
    "    return (df[col]*df[weight_col]).sum()/df[weight_col].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_std(df,columns,weight_col):\n",
    "    den = get_total(df,weight_col)\n",
    "    std_list = list()\n",
    "    \n",
    "    for col in columns:\n",
    "        mean = weighted_mean(df,col,weight_col)\n",
    "        diff = (df[col]-mean)**2\n",
    "        diff = diff*df[weight_col]\n",
    "        std_list.append(math.sqrt(diff.sum()/den))\n",
    "        \n",
    "    return std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coeff_df(df,model,columns):\n",
    "    coeff_df = pd.DataFrame(columns=['col','coeff','std','Coefficient'])\n",
    "    coeff_df['coeff'] = list(model.coef_[0])\n",
    "    coeff_df['col'] = columns\n",
    "    coeff_df['std'] = weighted_std(df,columns,weight_col)\n",
    "    coeff_df['Coefficient'] = abs(coeff_df['coeff']*coeff_df['std'])\n",
    "    coeff_df = coeff_df.sort_values(by='Coefficient',ascending=False)\n",
    "    coeff_df.drop(['coeff','std'],axis=1,inplace=True)\n",
    "    return coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression_model(df,target_col,weight_col,columns=None,reg=False,c=1):\n",
    "    train_df = make_copy_df(df)\n",
    "    \n",
    "    target = train_df[target_col]\n",
    "    train_df.drop([target_col],axis=1,inplace=True)\n",
    "    weight = train_df[weight_col]\n",
    "    train_df.drop([weight_col],axis=1,inplace=True)\n",
    "    \n",
    "    if columns is not None:\n",
    "        train_df.drop(columns,axis=1,inplace=True)\n",
    "    \n",
    "    if reg:\n",
    "        model = LogisticRegression(penalty='l1',C=c,verbose=1)\n",
    "    else:\n",
    "        model = LogisticRegression(verbose=1)\n",
    "        \n",
    "    model.fit(train_df,target,sample_weight=weight)\n",
    "    \n",
    "    train_df[target_col] = df[target_col]\n",
    "    train_df[weight_col] = df[weight_col]\n",
    "    coeff_df = get_coeff_df(train_df,model,get_features(train_df))\n",
    "    \n",
    "    return (model,coeff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model,coeff_df = fit_logistic_regression_model(numerical_df_without_na,target_col,weight_col,['os_id'],True,0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression_model_woe(df,target_col,weight_col,columns=None,reg=False,c=1):\n",
    "    train_df = make_copy_df(df)\n",
    "    \n",
    "    target = train_df[target_col]\n",
    "    train_df.drop([target_col],axis=1,inplace=True)\n",
    "    weight = train_df[weight_col]\n",
    "    train_df.drop([weight_col],axis=1,inplace=True)\n",
    "    \n",
    "    if columns is not None:\n",
    "        train_df.drop(columns,axis=1,inplace=True)\n",
    "    \n",
    "    if reg:\n",
    "        model = LogisticRegression(penalty='l1',C=c,verbose=1)\n",
    "    else:\n",
    "        model = LogisticRegression(verbose=1)\n",
    "        \n",
    "    model.fit(train_df,target,sample_weight=weight)\n",
    "    \n",
    "    train_df[target_col] = df[target_col]\n",
    "    train_df[weight_col] = df[weight_col]\n",
    "    coeff_df = pd.DataFrame(columns=['col','Coefficient'])\n",
    "    coeff_df['col'] = get_features(train_df)\n",
    "    coeff_df['Coefficient'] = abs(model.coef_[0])\n",
    "    coeff_df.sort_values(by='Coefficient',inplace=True,ascending=False)\n",
    "    \n",
    "    return (model,coeff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_woe,coeff_df_woe = fit_logistic_regression_model_woe(woe_df,target_col,weight_col,None,True,0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient DataFrame(Using WOE Encoded Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df_woe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Implementation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier,plot_importance,plot_tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Feature Importance\n",
    "def plot_feature_importance(model,importance_type):\n",
    "    plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "    plot_importance(model,importance_type=importance_type)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Tree Models\n",
    "def plot_and_save_boosted_trees(model,num_of_trees,filename,width,height):\n",
    "    plt.rcParams[\"figure.figsize\"] = [width,height]\n",
    "\n",
    "    i=0\n",
    "    for i in range(num_of_trees):\n",
    "        plot_tree(model,num_trees=i)\n",
    "        plt.savefig(filename+str(i)+\".png\",dpi=150)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features(model):\n",
    "    features_importance = model.get_booster().get_score(importance_type='cover')\n",
    "    sorted_features_importance = sorted(features_importance.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    \n",
    "    ret_list = list()\n",
    "    for i in range(len(sorted_features_importance)):\n",
    "        ret_list.append(sorted_features_importance[i][0])\n",
    "    \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgboost_model(df,target_col,weight_col):\n",
    "    df = make_copy_df(df)\n",
    "    \n",
    "    target = df[target_col]\n",
    "    df.drop([target_col],axis=1,inplace=True)\n",
    "    weight = df[weight_col]\n",
    "    df.drop([weight_col],axis=1,inplace=True)\n",
    "    \n",
    "    model = XGBClassifier(max_depth=5,learning_rate=0.1,n_estimators=10,missing=None,silent=False)\n",
    "    model.fit(df,target,sample_weight=weight)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df = make_numerical_df(misced_df,target_col,weight_col,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = fit_xgboost_model(numerical_df,target_col,weight_col)\n",
    "imp_feature_list_xgboost = get_important_features(xgboost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance(XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(xgboost_model,'cover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_data(df):\n",
    "    df = make_copy_df(df)\n",
    "    \n",
    "    list_le = dict()\n",
    "    for col in get_features(df):\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        list_le[col] = le\n",
    "        \n",
    "    return df,list_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoded_df,list_le = label_encode_data(misced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance_lightgbm(model):\n",
    "    plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "    lgb.plot_importance(model,importance_type='gain')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features_lightgbm(df,model):\n",
    "    ret_list = list(zip(list(df.columns),lightgbm_model.booster_.feature_importance(importance_type='gain')))\n",
    "    \n",
    "    return [x[0] for x in sorted(ret_list, key=lambda x: x[1],reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lightgbm_model(df,target_col,weight_col):\n",
    "    df = make_copy_df(df)\n",
    "    \n",
    "    target = df[target_col]\n",
    "    df.drop([target_col],axis=1,inplace=True)\n",
    "    weight = df[weight_col]\n",
    "    df.drop([weight_col],axis=1,inplace=True)\n",
    "    \n",
    "    model = lgb.LGBMClassifier(num_leaves=20,\n",
    "                               max_depth=5,learning_rate=0.01,objective='binary',n_estimators=100, max_cat_threshold = 500)\n",
    "    \n",
    "    model.fit(df,target,sample_weight=weight,categorical_feature=get_categorical_cols())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_model = fit_lightgbm_model(label_encoded_df,target_col,weight_col)\n",
    "imp_feature_list_lightgbm = get_important_features_lightgbm(misced_df,lightgbm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance_lightgbm(lightgbm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_mean_util(df,col_list,rank_dict):\n",
    "    col_set = set()\n",
    "    \n",
    "    i=1\n",
    "    for col in col_list:\n",
    "        rank_dict[col] = rank_dict[col]+i\n",
    "        col_set.add(col)\n",
    "        i +=1\n",
    "        \n",
    "    for col in get_features(df):\n",
    "        if col not in col_set:\n",
    "            rank_dict[col] +=i\n",
    "    \n",
    "    return rank_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_mean(df,coeff_df_woe,coeff_col,imp_feature_list_xgboost,imp_feature_list_lightgbm):\n",
    "    rank_dict = {}\n",
    "    for col in get_features(df):\n",
    "        rank_dict[col]=0\n",
    "        \n",
    "    rank_dict = rank_mean_util(df,list(coeff_df_woe[coeff_col]),rank_dict)\n",
    "    rank_dict = rank_mean_util(df,imp_feature_list_xgboost,rank_dict)\n",
    "    rank_dict = rank_mean_util(df,imp_feature_list_lightgbm,rank_dict)\n",
    "    \n",
    "    for col in rank_dict.keys():\n",
    "        rank_dict[col] = rank_dict[col]/3\n",
    "    \n",
    "    sorted_by_rank = sorted(rank_dict.items(), key=lambda kv: kv[1])\n",
    "    return [x[0] for x in sorted_by_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_list = rank_mean(misced_df,coeff_df_woe,'col',imp_feature_list_xgboost,imp_feature_list_lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_list(ret_list,columns=None):\n",
    "    if columns is not None:\n",
    "        return list(set(ret_list+columns))\n",
    "    else:\n",
    "        return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_list = ret_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_plot(misced_df,ret_list,weight_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Plot(Target Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_plot_target(misced_df,ret_list,target_col,weight_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Values Plot(Average Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_value_plot(misced_df,ret_list,target_col,weight_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Feature Importance(Next Candidate Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def conditional_feature_importance_util(df,target,weight,columns,model):\n",
    "    train_df = df[columns]\n",
    "    logloss_df = pd.DataFrame(columns=['Features','Logloss Reduction'])\n",
    "    \n",
    "    model.fit(train_df,target,sample_weight = weight)\n",
    "    predicted_target = model.predict_proba(train_df)\n",
    "    initial_logloss = log_loss(target,predicted_target,sample_weight = weight)\n",
    "    \n",
    "    for col in set(get_features(df))-set(columns):\n",
    "        train_df[col] = df[col]\n",
    "        \n",
    "        model.fit(train_df,target,sample_weight=weight)\n",
    "        predicted_target = model.predict_proba(train_df)\n",
    "        \n",
    "        logloss = log_loss(target,predicted_target,sample_weight = weight)\n",
    "        logloss_df = logloss_df.append({'Features':col,'Logloss Reduction':initial_logloss-logloss},ignore_index=True)\n",
    "        train_df.drop(col,axis=1,inplace=True)\n",
    "        \n",
    "    return logloss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_feature_importance(df,df_with_null,columns):\n",
    "    df = make_copy_df(df)\n",
    "    \n",
    "    target = df[target_col]\n",
    "    df.drop([target_col],axis=1,inplace=True)\n",
    "    weight = df[weight_col]\n",
    "    df.drop([weight_col],axis=1,inplace=True)\n",
    "    \n",
    "    model = LogisticRegression(penalty='l1',C=0.005,verbose=1)\n",
    "    logloss_logistic = conditional_feature_importance_util(df,target,weight,columns,model)\n",
    "    \n",
    "    model = XGBClassifier(learning_rate=0.1,max_depth=5,n_estimators=5,booster='gbtree',silent=False,missing=None)\n",
    "    logloss_xgboost = conditional_feature_importance_util(df_with_null,target,weight,columns,model)\n",
    "    \n",
    "    return (logloss_logistic,logloss_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(logloss_logistic,logloss_xgboost) = conditional_feature_importance(woe_df,numerical_df,['creative_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Reduction on addition of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_logistic = logloss_logistic.sort_values(by='Logloss Reduction',ascending=False)\n",
    "logloss_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_xgboost = logloss_xgboost.sort_values(by='Logloss Reduction',ascending=False)\n",
    "logloss_xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_bias_corrected(df,col1,col2,weight_col):\n",
    "    confusion_matrix = pd.crosstab(df[col1], df[col2], df[weight_col], aggfunc=sum)\n",
    "    confusion_matrix.fillna(0,inplace=True)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_corr_df(df,weight_col,target_col=None):\n",
    "    columns = get_categorical_cols()\n",
    "    if(len(columns)==1):\n",
    "        return\n",
    "    \n",
    "    if target_col is not None:\n",
    "        corr = pd.DataFrame(index=columns,columns=[target_col])\n",
    "        i=0\n",
    "        while i<len(columns):\n",
    "            corr.ix[columns[i]][target_col] = cramers_bias_corrected(df,columns[i],target_col,weight_col)\n",
    "            i+=1\n",
    "        corr.sort_values(by=target_col,ascending=False,inplace=True)\n",
    "    else:\n",
    "        corr = pd.DataFrame(index=columns,columns=columns)\n",
    "        \n",
    "        i=0\n",
    "        while i<len(columns):\n",
    "            j=i\n",
    "            while j<len(columns):\n",
    "                if i==j:\n",
    "                    corr.ix[columns[i]][columns[j]] = 1\n",
    "                else:\n",
    "                    corr.ix[columns[j]][columns[i]] = cramers_bias_corrected(df,columns[i],columns[j],weight_col)\n",
    "                    corr.ix[columns[i]][columns[j]] = corr.ix[columns[j]][columns[i]]\n",
    "                j+=1\n",
    "            i+=1\n",
    "            \n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_corr = categorical_corr_df(misced_df,weight_col)\n",
    "cat_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov(df, col1, col2, weight_col):\n",
    "    mean_col1 = weighted_mean(df,col1,weight_col)\n",
    "    mean_col2 = weighted_mean(df,col2,weight_col)\n",
    "    return ((df[weight_col]*((df[col1]-mean_col1)*(df[col2]-mean_col2))).sum())/get_total(df,weight_col)\n",
    "\n",
    "def get_pearson_coeff(df,col1,col2,weight_col):\n",
    "    num = cov(df,col1,col2,weight_col)\n",
    "    std1 = weighted_std(df,[col1],weight_col)[0]\n",
    "    std2 = weighted_std(df,[col2],weight_col)[0]\n",
    "    den = std1*std2\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_corr_df(df,weight_col,target_col=None):\n",
    "    #Change the below line to get_numerical_columns\n",
    "    columns = get_numerical_cols()\n",
    "    if(len(columns)==1):\n",
    "        return\n",
    "    \n",
    "    if target_col is not None:\n",
    "        corr = pd.DataFrame(index=columns,columns=[target_col])\n",
    "        i=0\n",
    "        while i<len(columns):\n",
    "            corr.ix[columns[i]][target_col] = get_pearson_coeff(df,columns[i],target_col,weight_col)\n",
    "            i+=1\n",
    "        corr.sort_values(by=target_col,ascending=False,inplace=True)\n",
    "    else:\n",
    "        corr = pd.DataFrame(index=columns,columns=columns)\n",
    "    \n",
    "        i=0\n",
    "        while i<len(columns):\n",
    "            j=i\n",
    "            while j<len(columns):\n",
    "                corr.ix[columns[j]][columns[i]] = get_pearson_coeff(df,columns[i],columns[j],weight_col)\n",
    "                corr.ix[columns[i]][columns[j]] = corr.ix[columns[j]][columns[i]]\n",
    "                j+=1\n",
    "            i+=1\n",
    "            \n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_corr = numerical_corr_df(misced_df,weight_col)\n",
    "num_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with Target\n",
    "corr_target = categorical_corr_df(misced_df,weight_col,target_col)\n",
    "corr_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Feature Values having similar target behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cluster(cluster_fig):\n",
    "    cluster_fig.render(view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_clustering(df,col,target_col,weight_col):\n",
    "    target = df[target_col]\n",
    "    weight = df[weight_col]\n",
    "    \n",
    "    train_df = pd.DataFrame(df[col])\n",
    "    \n",
    "    model = lgb.LGBMClassifier(boosting_type='gbdt',num_leaves=14,\n",
    "                               max_depth=4,learning_rate=1,objective='binary',n_estimators=1, max_cat_threshold = 1000)\n",
    "    \n",
    "    model.fit(train_df,target,sample_weight=weight,categorical_feature=col)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_cluster = lightgbm_clustering(label_encoded_df,[\"hour_id\"],target_col,weight_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_fig = lgb.create_tree_digraph(lightgbm_cluster,tree_index=0,filename='hour_tree',format='png')\n",
    "save_cluster(cluster_fig)\n",
    "cluster_fig"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}